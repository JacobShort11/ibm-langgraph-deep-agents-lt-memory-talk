{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Main Agent Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Middleware Overview:\n",
    "- PatchToolCallsMiddleware: This is default in-built middleware. Normalizes tool calls into the shape Deep Agents expects for \n",
    "  routing and message formatting. Built-in default to handle cases where the deep agent \n",
    "  is downstream of other agents.\n",
    "  \n",
    "- SummarizationMiddleware: This is default in-built middleware. Compacts conversation context before sending to the model when \n",
    "  token thresholds are exceeded, maintaining efficiency in long conversations.\n",
    "  \n",
    "- MemoryCleanupMiddleware: Prunes long-term memory to stay within token limits. This is \n",
    "  custom middleware we defined manually for this agent.\n",
    "\n",
    "Note: Using xray=1 to show sub-agent internal graphs. Without xray, sub-agents appear \n",
    "as tools and their internal structure is hidden.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '..')  # Add parent directory to path\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from agents.main_agent import agent\n",
    "\n",
    "# xray=1 shows the internal structure of sub-agents\n",
    "display(Image(agent.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Research Agent - Memory Management\n",
    "\n",
    "This notebook allows you to:\n",
    "1. View what's in the agent's long-term memory\n",
    "2. Add memories manually (e.g., pre-seed website quality ratings)\n",
    "3. Update or delete memories\n",
    "4. Search memories semantically\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook connects to the LangGraph server, which provides:\n",
    "- **Cloud-hosted store** - Persistent memory across sessions (same as LangSmith Studio)\n",
    "- **Cloud-hosted checkpointer** - Conversation state persistence\n",
    "\n",
    "### Setup Steps\n",
    "\n",
    "1. **Start the LangGraph server** (from the `deep-agent/` directory):\n",
    "   ```bash\n",
    "   cd deep-agent\n",
    "   langgraph dev\n",
    "   ```\n",
    "\n",
    "2. **Wait for the server** to be ready at `http://localhost:2024`\n",
    "\n",
    "3. **Run this notebook** - it will auto-discover your assistant configuration\n",
    "\n",
    "### Why This Architecture?\n",
    "\n",
    "When you run `langgraph dev`, LangGraph connects to LangSmith's cloud infrastructure for storage.\n",
    "This means:\n",
    "- Memories persist across sessions and are shared between this notebook and LangSmith Studio\n",
    "- No local PostgreSQL database required\n",
    "- The assistant_id is auto-discovered, so this notebook works for anyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from langgraph_sdk import get_sync_client\n",
    "\n",
    "# Connect to local LangGraph server (must have `langgraph dev` running)\n",
    "LANGGRAPH_URL = \"http://localhost:2024\"\n",
    "\n",
    "try:\n",
    "    client = get_sync_client(url=LANGGRAPH_URL)\n",
    "    # Quick health check\n",
    "    client.assistants.search(limit=1)\n",
    "    print(f\"Connected to LangGraph server at {LANGGRAPH_URL}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not connect to LangGraph server at {LANGGRAPH_URL}\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nMake sure to run 'langgraph dev' from the deep-agent/ directory first.\")\n",
    "    raise SystemExit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-discover the assistant namespace\n",
    "# This ensures the notebook works for anyone, regardless of their assistant_id\n",
    "\n",
    "namespaces = client.store.list_namespaces()\n",
    "filesystem_namespaces = [ns for ns in namespaces[\"namespaces\"] if len(ns) == 2 and ns[1] == \"filesystem\"]\n",
    "\n",
    "if filesystem_namespaces:\n",
    "    NAMESPACE = filesystem_namespaces[0]\n",
    "    ASSISTANT_ID = NAMESPACE[0]\n",
    "    print(f\"Auto-discovered assistant: {ASSISTANT_ID}\")\n",
    "    print(f\"Using namespace: {NAMESPACE}\")\n",
    "else:\n",
    "    print(\"No assistant namespace found.\")\n",
    "    print(\"\\nThis usually means:\")\n",
    "    print(\"  1. You haven't run the agent yet in LangSmith Studio, OR\")\n",
    "    print(\"  2. The agent hasn't written any memories yet\")\n",
    "    print(\"\\nTry running a query in LangSmith Studio first, then re-run this cell.\")\n",
    "    NAMESPACE = None\n",
    "    ASSISTANT_ID = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Namespace Structure\n",
    "\n",
    "The agent uses a namespace of `[assistant_id, \"filesystem\"]` for persistent storage.\n",
    "\n",
    "Within this namespace, memory files are stored as:\n",
    "- `/website_quality.txt` - Website reliability ratings\n",
    "- `/research_lessons.txt` - What approaches worked well\n",
    "- `/source_notes.txt` - Notes about specific sources\n",
    "- `/coding.txt` - Code mistakes and lessons\n",
    "\n",
    "**Note**: The `/memories/` prefix from the agent's perspective is stripped by the CompositeBackend routing, so files are stored without it in the actual store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. View All Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_memories():\n",
    "    \"\"\"List all memories in the agent's namespace.\"\"\"\n",
    "    if not NAMESPACE:\n",
    "        print(\"No namespace configured. Run the setup cells first.\")\n",
    "        return []\n",
    "    \n",
    "    response = client.store.search_items(NAMESPACE, limit=100)\n",
    "    items = response[\"items\"]\n",
    "    \n",
    "    if not items:\n",
    "        print(\"No memories found\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(items)} memories:\\n\")\n",
    "    for i, item in enumerate(items, 1):\n",
    "        print(f\"--- Memory {i} ---\")\n",
    "        print(f\"Key: {item['key']}\")\n",
    "        print(f\"Created: {item['created_at']}\")\n",
    "        print(f\"Updated: {item['updated_at']}\")\n",
    "        \n",
    "        # Handle content format (list of lines)\n",
    "        value = item[\"value\"]\n",
    "        if isinstance(value.get(\"content\"), list):\n",
    "            content = \"\\n\".join(value[\"content\"])\n",
    "        else:\n",
    "            content = json.dumps(value, indent=2)\n",
    "        \n",
    "        if len(content) > 500:\n",
    "            print(f\"Content: {content[:500]}...\")\n",
    "        else:\n",
    "            print(f\"Content: {content}\")\n",
    "        print()\n",
    "    \n",
    "    return items\n",
    "\n",
    "# List all memories\n",
    "all_memories = list_all_memories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. View Specific Memory File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory(key: str):\n",
    "    \"\"\"Get a specific memory by key.\"\"\"\n",
    "    if not NAMESPACE:\n",
    "        print(\"No namespace configured. Run the setup cells first.\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure key starts with /\n",
    "    if not key.startswith(\"/\"):\n",
    "        key = f\"/{key}\"\n",
    "    \n",
    "    try:\n",
    "        item = client.store.get_item(NAMESPACE, key)\n",
    "        if item:\n",
    "            print(f\"Memory: {key}\")\n",
    "            print(f\"Created: {item['created_at']}\")\n",
    "            print(f\"Updated: {item['updated_at']}\")\n",
    "            \n",
    "            value = item[\"value\"]\n",
    "            if isinstance(value.get(\"content\"), list):\n",
    "                content = \"\\n\".join(value[\"content\"])\n",
    "            else:\n",
    "                content = json.dumps(value, indent=2)\n",
    "            \n",
    "            print(f\"\\nContent:\\n{content}\")\n",
    "            return item\n",
    "        else:\n",
    "            print(f\"Memory '{key}' not found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example: Get website quality memory\n",
    "get_memory(\"website_quality.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save/Overwrite Memories\n",
    "\n",
    "Use this to pre-seed the agent with memories or update existing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_memory(key: str, content: str):\n",
    "    \"\"\"Save or update a memory.\"\"\"\n",
    "    if not NAMESPACE:\n",
    "        print(\"No namespace configured. Run the setup cells first.\")\n",
    "        return\n",
    "    \n",
    "    # Ensure key starts with /\n",
    "    if not key.startswith(\"/\"):\n",
    "        key = f\"/{key}\"\n",
    "    \n",
    "    timestamp = datetime.now().isoformat()\n",
    "    client.store.put_item(\n",
    "        NAMESPACE,\n",
    "        key,\n",
    "        value={\n",
    "            \"content\": content.split(\"\\n\"),\n",
    "            \"created_at\": timestamp,\n",
    "            \"modified_at\": timestamp,\n",
    "        }\n",
    "    )\n",
    "    print(f\"Saved memory: {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed all 4 memory files with realistic content\n",
    "\n",
    "# Memory 1: Website quality discoveries\n",
    "save_memory(\"website_quality.txt\", \"\"\"# Website Quality Ratings\n",
    "Last updated: 2025-03-05\n",
    "\n",
    "## Discovered High Quality Sources\n",
    "- reuters.com (5/5) - Fast, timestamped equity headlines; strong for M&A and earnings timing\n",
    "- ft.com (4/5) - Good macro/regulatory context; cross-check figures against filings due to paywall summaries\n",
    "- sec.gov/edgar (5/5) - Primary filings and 8-Ks; reliable event timestamps\n",
    "- company investor relations sites (4/5) - Earnings releases, decks, transcripts; marketing tone but primary data\n",
    "- bls.gov / bea.gov (5/5) - Official macro prints; cite release time in ET\n",
    "\n",
    "## Sources That Disappointed\n",
    "- seekingalpha.com (2/5) - Opinion-heavy; numbers often rounded or missing timestamps\n",
    "- yahoo.com/finance aggregated news (3/5) - Rewrites with delayed timestamps; verify against originals\n",
    "- marketbeat.com (2/5) - Promotional bias around analyst ratings\n",
    "- twitter.com/x.com (2/5) - Sentiment only; not dependable for factual claims\n",
    "\n",
    "## Domain-Specific Findings\n",
    "- For earnings: start with 8-K + press release; use Reuters for corroborating headline times\n",
    "- For guidance changes: check IR decks and follow-up 8-K exhibits; call transcripts add nuance\n",
    "- For macro prints: use BLS/BEA/Fed releases; supplement with Reuters for market color\n",
    "- For regulatory actions: use sec.gov, ftc.gov, and ec.europa.eu press rooms for primary statements\n",
    "\"\"\")\n",
    "\n",
    "# Memory 2: Research strategies that worked\n",
    "save_memory(\"research_lessons.txt\", \"\"\"# Research Lessons Learned\n",
    "Last updated: 2025-03-05\n",
    "\n",
    "## Effective Search Patterns\n",
    "- Use \"TICKER earnings 8-K\" and \"TICKER investor relations presentation\" to get primary documents fast\n",
    "- Add `site:reuters.com TICKER` with the last two dates to capture timestamped headlines in the 48h window\n",
    "- Combine sector term + \"regulation\" + current year to surface policy moves (e.g., \"semiconductor export rules 2025\")\n",
    "- Pair company + supplier/peer names to uncover supply chain impacts\n",
    "- Pull econ releases via \"BLS CPI release\" or \"FOMC statement PDF\" to land on official links\n",
    "\n",
    "## What Didn't Work\n",
    "- Generic \"company news\" queries return SEO blogs with stale dates\n",
    "- Analyst note summaries without tickers miss context; need original note or wire copy\n",
    "- Social clips about rumors rarely cite sources; avoid unless corroborated\n",
    "\n",
    "## Verification Strategies\n",
    "- Cross-check headline timestamp vs filing time to align with price move window\n",
    "- For price reactions, anchor to local market hours and convert all times to GMT/ET consistently\n",
    "- Require at least two independent wires for big claims (M&A, investigations, outages)\n",
    "- Save raw links plus a short note on what was confirmed to speed credibility checks later\n",
    "\"\"\")\n",
    "\n",
    "# Memory 3: Specific source notes\n",
    "save_memory(\"source_notes.txt\", \"\"\"# Source Notes\n",
    "\n",
    "## reuters.com\n",
    "- Datelines reliable; includes author + timestamp; good for quick double-source on filings\n",
    "- Often provides context on sector peers and analyst quotes\n",
    "\n",
    "## sec.gov/edgar\n",
    "- Use 8-K for unscheduled news; 10-Q/10-K for numbers; timestamps are ET\n",
    "- Download exhibits for slides; some data only lives in attachments\n",
    "\n",
    "## company investor relations\n",
    "- Press releases and slide decks usually post within minutes of EDGAR filing\n",
    "- Earnings call transcripts (company-hosted or third-party) need verification against audio for quotes\n",
    "\n",
    "## ft.com\n",
    "- Strong for regulatory and geopolitical context affecting sectors\n",
    "- Paywalled; summarize but verify figures against primary releases\n",
    "\"\"\")\n",
    "\n",
    "# Memory 4: Coding mistakes and lessons\n",
    "save_memory(\"coding.txt\", \"\"\"# Coding Lessons\n",
    "Last updated: 2025-03-05\n",
    "\n",
    "## Common Pitfalls\n",
    "- Normalize timestamps to timezone-aware UTC, then convert to ET/GMT for market context\n",
    "- Align price series to market sessions before calculating event-window returns\n",
    "- Trim outliers and low-liquidity periods; check volume to avoid false spikes\n",
    "- Close matplotlib figures after saving to prevent memory leaks in repeated runs\n",
    "\n",
    "## Debugging Tips\n",
    "- Log dataframe shapes after merges; mismatched tickers often drop rows silently\n",
    "- Plot price and volume around events to confirm the move is real\n",
    "- When reading CSVs from scratchpad, use parse_dates to avoid string time issues\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nAll seed memories created!\")\n",
    "list_all_memories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Append to Existing Memory\n",
    "\n",
    "Add new learnings to an existing memory file without overwriting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_memory(key: str, new_content: str):\n",
    "    \"\"\"Append content to an existing memory.\"\"\"\n",
    "    if not NAMESPACE:\n",
    "        print(\"No namespace configured. Run the setup cells first.\")\n",
    "        return\n",
    "    \n",
    "    if not key.startswith(\"/\"):\n",
    "        key = f\"/{key}\"\n",
    "    \n",
    "    try:\n",
    "        existing = client.store.get_item(NAMESPACE, key)\n",
    "        if existing and \"content\" in existing[\"value\"]:\n",
    "            current_lines = existing[\"value\"][\"content\"]\n",
    "            new_lines = new_content.split(\"\\n\")\n",
    "            updated_lines = current_lines + [\"\"] + new_lines\n",
    "            updated_content = \"\\n\".join(updated_lines)\n",
    "        else:\n",
    "            updated_content = new_content\n",
    "        \n",
    "        save_memory(key, updated_content)\n",
    "    except Exception:\n",
    "        # File doesn't exist, create it\n",
    "        save_memory(key, new_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Append a new discovery to research lessons\n",
    "append_to_memory(\"research_lessons.txt\", \"\"\"\n",
    "## New Discovery (added manually)\n",
    "- \"Ticker + premarket\" queries surface why moves start before the open; pair with time-filtered Reuters hits\n",
    "- EDGAR 'Latest Filings' page is the fastest way to confirm unscheduled 8-Ks after-hours\n",
    "\"\"\")\n",
    "\n",
    "print(\"Appended to research_lessons.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_memory(\"research_lessons.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Delete Memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_memory(key: str):\n",
    "    \"\"\"Delete a specific memory.\"\"\"\n",
    "    if not NAMESPACE:\n",
    "        print(\"No namespace configured. Run the setup cells first.\")\n",
    "        return\n",
    "    \n",
    "    if not key.startswith(\"/\"):\n",
    "        key = f\"/{key}\"\n",
    "    \n",
    "    client.store.delete_item(NAMESPACE, key)\n",
    "    print(f\"Deleted: {key}\")\n",
    "\n",
    "def clear_all_memories():\n",
    "    \"\"\"Delete ALL memories (use with caution!).\"\"\"\n",
    "    if not NAMESPACE:\n",
    "        print(\"No namespace configured. Run the setup cells first.\")\n",
    "        return\n",
    "    \n",
    "    response = client.store.search_items(NAMESPACE, limit=100)\n",
    "    items = response[\"items\"]\n",
    "    \n",
    "    for item in items:\n",
    "        client.store.delete_item(NAMESPACE, item[\"key\"])\n",
    "    \n",
    "    print(f\"Cleared {len(items)} memories\")\n",
    "\n",
    "# Example:\n",
    "# delete_memory(\"coding.txt\")  # Delete specific file\n",
    "# clear_all_memories()  # Delete everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Reference\n",
    "\n",
    "```python\n",
    "# List all memories\n",
    "list_all_memories()\n",
    "\n",
    "# Get specific memory\n",
    "get_memory(\"website_quality.txt\")\n",
    "\n",
    "# Save/overwrite memory\n",
    "save_memory(\"coding.txt\", \"# My content\\n- bullet 1\\n- bullet 2\")\n",
    "\n",
    "# Append to memory\n",
    "append_to_memory(\"research_lessons.txt\", \"## New section\\n- new lesson\")\n",
    "\n",
    "# Delete memory\n",
    "delete_memory(\"coding.txt\")\n",
    "\n",
    "# Clear all (careful!)\n",
    "clear_all_memories()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

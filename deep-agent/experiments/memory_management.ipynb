{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Agent Memory Management\n",
    "\n",
    "Manage long-term memories for your LangGraph agents. Changes here sync with LangSmith Studio.\n",
    "\n",
    "**Setup**: Run `langgraph dev` from `deep-agent/` first.\n",
    "\n",
    "**Graphs**: `main-agent`, `analysis-agent`, `web-research-agent`, `credibility-agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to LangGraph server at http://localhost:2024\n",
      "Available graphs: ['analysis-agent', 'credibility-agent', 'web-research-agent', 'main-agent']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from langgraph_sdk import get_sync_client\n",
    "\n",
    "LANGGRAPH_URL = \"http://localhost:2024\"\n",
    "\n",
    "try:\n",
    "    client = get_sync_client(url=LANGGRAPH_URL)\n",
    "    all_assistants = client.assistants.search(limit=100)\n",
    "    print(f\"Connected to LangGraph server at {LANGGRAPH_URL}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not connect: {e}\\nRun 'langgraph dev' from the deep-agent/ directory first.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# Discover graphs from existing assistants\n",
    "AVAILABLE_GRAPHS = list(set(a[\"graph_id\"] for a in all_assistants if a.get(\"graph_id\")))\n",
    "if not AVAILABLE_GRAPHS:\n",
    "    AVAILABLE_GRAPHS = [\"main-agent\", \"analysis-agent\", \"web-research-agent\", \"credibility-agent\"]\n",
    "print(f\"Available graphs: {AVAILABLE_GRAPHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "Each graph gets a default assistant from `langgraph dev`. We find those assistants and use their namespaces `[assistant_id, \"filesystem\"]` to read/write memories. Same IDs as LangSmith Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysis-agent: 854aa6d0-4ddb-5980-837f-8db6045be50f\n",
      "credibility-agent: 91e1bc6c-3ce8-58e1-9506-d229476e35f8\n",
      "web-research-agent: 3e46236d-ede5-5fd4-91d9-6fd4977ad898\n",
      "main-agent: 9cbab8c9-1a30-54df-a839-d7e26aca6464\n"
     ]
    }
   ],
   "source": [
    "# Cache assistants and build namespace lookup\n",
    "ASSISTANTS = {}\n",
    "for graph in AVAILABLE_GRAPHS:\n",
    "    assistants = client.assistants.search(graph_id=graph, limit=1)\n",
    "    if assistants:\n",
    "        ASSISTANTS[graph] = assistants[0]\n",
    "        print(f\"{graph}: {assistants[0]['assistant_id']}\")\n",
    "\n",
    "def get_namespace(graph: str) -> tuple:\n",
    "    \"\"\"Get the memory namespace for a graph.\"\"\"\n",
    "    if graph not in ASSISTANTS:\n",
    "        raise ValueError(f\"No assistant for '{graph}'. Run it in LangSmith Studio first.\")\n",
    "    return (ASSISTANTS[graph][\"assistant_id\"], \"filesystem\")\n",
    "\n",
    "def normalize_key(key: str) -> str:\n",
    "    \"\"\"Ensure key starts with /\"\"\"\n",
    "    return key if key.startswith(\"/\") else f\"/{key}\"\n",
    "\n",
    "def format_content(value: dict) -> str:\n",
    "    \"\"\"Format memory content for display.\"\"\"\n",
    "    if isinstance(value.get(\"content\"), list):\n",
    "        return \"\\n\".join(value[\"content\"])\n",
    "    return json.dumps(value, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Memories\n",
    "\n",
    "List all memories for a graph or get a specific file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== main-agent (2 memories) ===\n",
      "\n",
      "[/source_notes.txt]\n",
      "## Source Notes\n",
      "- Treat ts2.tech as a secondary/tertiary commentary site; use it only for context and always verify factual assertions (dates, quantities, prices) against primary filings or Tier-1 newswires.\n",
      "\n",
      "\n",
      "[/website_quality.txt]\n",
      "## Website Quality Notes\n",
      "- Yahoo Finance press-release pages are generally reliable mirrors of company/PR wire announcements but should be cross-checked with original RNS/Investegate or company site when confirming precise deal terms.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'namespace': ['9cbab8c9-1a30-54df-a839-d7e26aca6464', 'filesystem'],\n",
       "  'key': '/source_notes.txt',\n",
       "  'value': {'content': ['## Source Notes',\n",
       "    '- Treat ts2.tech as a secondary/tertiary commentary site; use it only for context and always verify factual assertions (dates, quantities, prices) against primary filings or Tier-1 newswires.',\n",
       "    ''],\n",
       "   'created_at': '2025-12-24T10:51:16.945366+00:00',\n",
       "   'modified_at': '2025-12-24T10:51:16.945366+00:00'},\n",
       "  'created_at': '2025-12-24T10:51:16.945559+00:00',\n",
       "  'updated_at': '2025-12-24T10:51:16.945563+00:00',\n",
       "  'score': None},\n",
       " {'namespace': ['9cbab8c9-1a30-54df-a839-d7e26aca6464', 'filesystem'],\n",
       "  'key': '/website_quality.txt',\n",
       "  'value': {'content': ['## Website Quality Notes',\n",
       "    '- Yahoo Finance press-release pages are generally reliable mirrors of company/PR wire announcements but should be cross-checked with original RNS/Investegate or company site when confirming precise deal terms.',\n",
       "    ''],\n",
       "   'created_at': '2025-12-24T10:51:16.945482+00:00',\n",
       "   'modified_at': '2025-12-24T10:51:16.945482+00:00'},\n",
       "  'created_at': '2025-12-24T10:51:16.945767+00:00',\n",
       "  'updated_at': '2025-12-24T10:51:16.945768+00:00',\n",
       "  'score': None}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def list_memories(graph: str = \"main-agent\"):\n",
    "    \"\"\"List all memories for a graph.\"\"\"\n",
    "    items = client.store.search_items(get_namespace(graph), limit=100)[\"items\"]\n",
    "    \n",
    "    print(f\"\\n=== {graph} ({len(items)} memories) ===\\n\")\n",
    "    for item in items:\n",
    "        content = format_content(item[\"value\"])\n",
    "        preview = content[:300] + \"...\" if len(content) > 300 else content\n",
    "        print(f\"[{item['key']}]\\n{preview}\\n\")\n",
    "    return items\n",
    "\n",
    "# Example\n",
    "list_memories(\"main-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main-agent] website_quality.txt\n",
      "## Website Quality Notes\n",
      "- Yahoo Finance press-release pages are generally reliable mirrors of company/PR wire announcements but should be cross-checked with original RNS/Investegate or company site when confirming precise deal terms.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'namespace': ['9cbab8c9-1a30-54df-a839-d7e26aca6464', 'filesystem'],\n",
       " 'key': '/website_quality.txt',\n",
       " 'value': {'content': ['## Website Quality Notes',\n",
       "   '- Yahoo Finance press-release pages are generally reliable mirrors of company/PR wire announcements but should be cross-checked with original RNS/Investegate or company site when confirming precise deal terms.',\n",
       "   ''],\n",
       "  'created_at': '2025-12-24T10:51:16.945482+00:00',\n",
       "  'modified_at': '2025-12-24T10:51:16.945482+00:00'},\n",
       " 'created_at': '2025-12-24T10:51:16.945767+00:00',\n",
       " 'updated_at': '2025-12-24T10:51:16.945768+00:00'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_memory(key: str, graph: str = \"main-agent\"):\n",
    "    \"\"\"Get a specific memory by key.\"\"\"\n",
    "    item = client.store.get_item(get_namespace(graph), normalize_key(key))\n",
    "    if item:\n",
    "        print(f\"[{graph}] {key}\\n{format_content(item['value'])}\")\n",
    "        return item\n",
    "    print(f\"Not found: {key}\")\n",
    "    return None\n",
    "\n",
    "# Example\n",
    "get_memory(\"website_quality.txt\", \"main-agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Update Memories\n",
    "\n",
    "Create new memories, overwrite existing ones, append content, or seed defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_memory(key: str, content: str, graph: str = \"main-agent\"):\n",
    "    \"\"\"Save or update a memory.\"\"\"\n",
    "    key = normalize_key(key)\n",
    "    ts = datetime.now().isoformat()\n",
    "    client.store.put_item(\n",
    "        get_namespace(graph), key,\n",
    "        value={\"content\": content.split(\"\\n\"), \"created_at\": ts, \"modified_at\": ts}\n",
    "    )\n",
    "    print(f\"Saved: {key} ({graph})\")\n",
    "\n",
    "def append_memory(key: str, content: str, graph: str = \"main-agent\"):\n",
    "    \"\"\"Append content to an existing memory.\"\"\"\n",
    "    key = normalize_key(key)\n",
    "    existing = client.store.get_item(get_namespace(graph), key)\n",
    "    if existing and \"content\" in existing[\"value\"]:\n",
    "        current = \"\\n\".join(existing[\"value\"][\"content\"])\n",
    "        content = current + \"\\n\\n\" + content\n",
    "    save_memory(key, content, graph)\n",
    "\n",
    "SEED_MEMORIES = {\n",
    "    \"website_quality.txt\": \"\"\"# Website Quality Ratings\n",
    "\n",
    "## Tier 1 - Primary Sources (5/5)\n",
    "- sec.gov/edgar - 8-K for material events, 10-Q/10-K for financials, DEF 14A for proxy/governance\n",
    "- federalreserve.gov - FOMC statements, minutes, dot plots, Beige Book\n",
    "- bls.gov / bea.gov - NFP, CPI, GDP, PCE; always use seasonally adjusted figures\n",
    "- treasury.gov - Auction results, TIC data, debt ceiling updates\n",
    "- ecb.europa.eu / bankofengland.co.uk - Rate decisions, forward guidance\n",
    "\n",
    "## Tier 2 - Reliable News (4/5)\n",
    "- reuters.com - Timestamped headlines, strong for M&A/earnings breaks, minimal lag\n",
    "- bloomberg.com - Excellent for rates/FX, often first on Fed leaks (paywalled)\n",
    "- ft.com - Strong regulatory/policy context, UK/EU focus (paywalled)\n",
    "- wsj.com - Corporate governance scoops, Fed coverage (paywalled)\n",
    "\n",
    "## Tier 3 - Useful with Verification (3/5)\n",
    "- yahoo.finance.com - Good for PR wire mirrors, verify deal terms against RNS/Investegate\n",
    "- cnbc.com - Fast on breaking news but verify timestamps independently\n",
    "- marketwatch.com - Decent for earnings recaps, cross-check numbers\n",
    "\n",
    "## Tier 4 - Use Cautiously (2/5)\n",
    "- seekingalpha.com - Opinion-heavy, useful for sentiment not facts\n",
    "- investopedia.com - Educational only, not for current events\n",
    "- twitter.com/x.com - Sentiment gauge only, never cite as source\n",
    "\n",
    "## Avoid\n",
    "- motleyfool.com - Clickbait, affiliate-driven\n",
    "- benzinga.com - Often recycled PR with added spin\n",
    "- Generic SEO finance blogs - No editorial standards\"\"\",\n",
    "\n",
    "    \"research_lessons.txt\": \"\"\"# Research Lessons\n",
    "\n",
    "## Task Decomposition That Works\n",
    "- ONE stock/topic per web-research-agent call prevents exhausted tool limits\n",
    "- Parallel calls for independent stocks, sequential for dependent analysis\n",
    "- Ask for \"5-10 key findings with sources\" not \"comprehensive research\"\n",
    "- Time-bound queries: \"TICKER Q3 2024 earnings\" beats \"TICKER recent earnings\"\n",
    "\n",
    "## Cross-Referencing Best Practices\n",
    "- Filing timestamp vs headline timestamp reveals information asymmetry\n",
    "- Compare 8-K language to earnings call transcript for management spin\n",
    "- Check short interest (via SEC 13F) when bearish narratives emerge\n",
    "- Verify M&A terms: press release vs 8-K vs DEFM14A can differ\n",
    "\n",
    "## What Didn't Work\n",
    "- \"COMPANY news\" returns SEO blogs and aggregators\n",
    "- Asking sub-agents for \"everything about X\" leads to incomplete returns\n",
    "- Social media clips rarely cite primary sources\n",
    "- Generic macro queries without date constraints return stale data\n",
    "- Combining multiple stocks in one sub-agent call causes truncated results\n",
    "\n",
    "## Synthesis Lessons\n",
    "- Lead with the trading implication, then provide evidence\n",
    "- Note confidence levels: high (primary source), medium (Tier 2), low (single source)\n",
    "- Flag when bullish/bearish narratives conflict across sources\n",
    "- Time-stamp everything relative to market hours (ET)\"\"\",\n",
    "\n",
    "    \"source_notes.txt\": \"\"\"# Source Notes\n",
    "\n",
    "## SEC Filing Types\n",
    "- 8-K: Material events (earnings, M&A, exec changes, guidance) - most time-sensitive\n",
    "- 10-Q: Quarterly financials, MD&A, risk factors - numbers to analyze\n",
    "- 10-K: Annual comprehensive, audited - baseline for deep dives\n",
    "- DEF 14A: Proxy statements, exec comp, governance - activism angles\n",
    "- 13F: Institutional holdings - quarterly, 45-day lag\n",
    "- SC 13D/G: 5%+ ownership changes - activist positions\n",
    "\n",
    "## News Wire Hierarchy\n",
    "- Company IR page > PR Newswire/Business Wire > Reuters/Bloomberg > Secondary coverage\n",
    "- Always prefer the source closest to the company for quotes and figures\n",
    "\n",
    "## Macro Data Release Schedule\n",
    "- NFP: First Friday of month, 8:30 ET\n",
    "- CPI: ~12th of month, 8:30 ET\n",
    "- FOMC: 8 meetings/year, statement at 2:00 ET, minutes 3 weeks later\n",
    "- GDP: Advance (1mo), Second (2mo), Third (3mo) estimates\n",
    "\n",
    "## Cross-Asset Signal Sources\n",
    "- CME FedWatch for rate probabilities\n",
    "- MOVE index for rates volatility\n",
    "- VIX for equity volatility\n",
    "- DXY for dollar strength\n",
    "- Credit spreads (CDX IG/HY) for risk appetite\"\"\",\n",
    "\n",
    "    \"coding.txt\": \"\"\"# Coding Lessons\n",
    "\n",
    "## Time Series Best Practices\n",
    "- Normalize all timestamps to UTC, convert to ET for market context display\n",
    "- Use pandas Timestamp with tz_localize/tz_convert, never naive datetimes\n",
    "- Align price data by date before computing returns or correlations\n",
    "- Forward-fill missing prices (holidays), but flag gaps > 3 days\n",
    "\n",
    "## Visualization Standards\n",
    "- Use plt.style.use('seaborn-v0_8-whitegrid') for clean financial charts\n",
    "- Always label axes with units (%, $, bps)\n",
    "- Include date range in title: \"AAPL Daily Returns (Jan-Dec 2024)\"\n",
    "- Use consistent colors: green=positive, red=negative, blue=benchmark\n",
    "- Close figures after saving: plt.close(fig) prevents memory leaks\n",
    "\n",
    "## Data Validation\n",
    "- Log dataframe shapes after every merge/join operation\n",
    "- Check for NaN/inf before any calculation\n",
    "- Verify date ranges match across datasets before correlation analysis\n",
    "- Sanity check: daily returns > 20% or < -20% should trigger investigation\n",
    "\n",
    "## Gradient Boosting for Time Series\n",
    "- Create lag features (t-1, t-2, t-5, t-20) for momentum signals\n",
    "- Include rolling means (5d, 20d, 50d) and rolling std for volatility\n",
    "- Train/test split by time, never shuffle time series\n",
    "- Use at least 100+ data points for meaningful patterns\n",
    "\n",
    "## Output Formatting\n",
    "- Round percentages to 2 decimal places\n",
    "- Use locale-aware number formatting for large values (1,234,567)\n",
    "- Return plot URLs immediately after saving, don't batch\"\"\"\n",
    "}\n",
    "\n",
    "def seed_memories(graph: str = \"main-agent\"):\n",
    "    \"\"\"Seed default memories for a graph.\"\"\"\n",
    "    for key, content in SEED_MEMORIES.items():\n",
    "        save_memory(key, content, graph)\n",
    "    print(f\"Seeded {len(SEED_MEMORIES)} memories for '{graph}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /website_quality.txt (analysis-agent)\n",
      "Saved: /research_lessons.txt (analysis-agent)\n",
      "Saved: /source_notes.txt (analysis-agent)\n",
      "Saved: /coding.txt (analysis-agent)\n",
      "Seeded 4 memories for 'analysis-agent'\n",
      "Saved: /website_quality.txt (credibility-agent)\n",
      "Saved: /research_lessons.txt (credibility-agent)\n",
      "Saved: /source_notes.txt (credibility-agent)\n",
      "Saved: /coding.txt (credibility-agent)\n",
      "Seeded 4 memories for 'credibility-agent'\n",
      "Saved: /website_quality.txt (web-research-agent)\n",
      "Saved: /research_lessons.txt (web-research-agent)\n",
      "Saved: /source_notes.txt (web-research-agent)\n",
      "Saved: /coding.txt (web-research-agent)\n",
      "Seeded 4 memories for 'web-research-agent'\n",
      "Saved: /website_quality.txt (main-agent)\n",
      "Saved: /research_lessons.txt (main-agent)\n",
      "Saved: /source_notes.txt (main-agent)\n",
      "Saved: /coding.txt (main-agent)\n",
      "Seeded 4 memories for 'main-agent'\n"
     ]
    }
   ],
   "source": [
    "# Seed all graphs: \n",
    "# for graph in AVAILABLE_GRAPHS: seed_memories(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Memories\n",
    "\n",
    "Remove individual memories or clear all for a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_memory(key: str, graph: str = \"main-agent\"):\n",
    "    \"\"\"Delete a specific memory.\"\"\"\n",
    "    client.store.delete_item(get_namespace(graph), normalize_key(key))\n",
    "    print(f\"Deleted: {key} ({graph})\")\n",
    "\n",
    "def clear_memories(graph: str = \"main-agent\"):\n",
    "    \"\"\"Delete ALL memories for a graph.\"\"\"\n",
    "    ns = get_namespace(graph)\n",
    "    items = client.store.search_items(ns, limit=100)[\"items\"]\n",
    "    for item in items:\n",
    "        client.store.delete_item(ns, item[\"key\"])\n",
    "    print(f\"Cleared {len(items)} memories ({graph})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "```python\n",
    "AVAILABLE_GRAPHS  # [\"main-agent\", \"analysis-agent\", \"web-research-agent\", \"credibility-agent\"]\n",
    "\n",
    "list_memories(\"main-agent\")                          # List all memories\n",
    "get_memory(\"website_quality.txt\", \"main-agent\")      # Get one memory\n",
    "\n",
    "save_memory(\"file.txt\", \"content\", \"main-agent\")     # Save/overwrite\n",
    "append_memory(\"file.txt\", \"more\", \"main-agent\")      # Append\n",
    "seed_memories(\"main-agent\")                          # Seed defaults\n",
    "\n",
    "delete_memory(\"file.txt\", \"main-agent\")              # Delete one\n",
    "clear_memories(\"main-agent\")                         # Clear graph\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
